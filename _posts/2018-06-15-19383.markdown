---
layout: post
title: Police Use of Minority Report-Style Pre-Crime Tech Raises Inaccuracy Concerns
date: 2018-06-15 20:15:58
tourl: https://www.bleepingcomputer.com/news/government/police-use-of-minority-report-style-pre-crime-tech-raises-inaccuracy-concerns/
tags: [law]
---
Pre-crime, is a vast potpourri of information, on everyday activities, used to try to predict and prevent future behavior. In predictive policing, computer algorithms identify signs ofHartford, CT is now using what some BriefCam, An Israeli-American cyber business, provides video search technology that is now being utilized in Hartford. BriefCam's video analysis software compresses hours of video into more compact presentations, referred to as "events." In cities comparable to Hartford, with at leastLaw enforcement's foray into utilizing this type of technology is not without its critics. Local residents and state chapters of the ACLU are uneasy with police departments' use of technology for which they say the endgame seems disturbingly clear.Currently being employed in Fresno, CA, one example of the use of pre-crime technology is described by the "...officers raced to a recent 911 call about a man threatening his ex-girlfriend, a police operator in headquarters consulted software that scored the suspects potential for violence the way a bank might run a credit report.The program scoured billions of data points, including arrest reports, property records, commercial databases, deep Web searches and the mans social- media postings. It calculated his threat level as the highest of three color-coded scores: a bright red warning.The man had a firearm conviction and gang associations, so out of caution police called a negotiator. The suspect surrendered, and police said the intelligence helped them make the right call  it turned out he had a gun."In Chicago, the police have been applying machine learning and predictive analytics to police data sets, including crime incidents, arrests, and weather data. When data such as previous arrest records is combined with real-time IoT data, (i.e. sensor-influenced cameras that detect gunshots) it becomes easier to ascertain problem locations. Known as the pre-crime initiative, it was implemented through The software used is HunchLab, a geographic prediction tool that employs data modeling to predict risk in specific areas across the city. At-risk regions are highlighted on-screen, while recommendations for action are displayed alongside the at-risk region information. The information is then collated into a decision support system and made accessible to individual police officers on the beat.Adoption of pre-crime tech is beginning to trend in the US. PredPol, one of the leading systems on the market, is already being used by law enforcement in California, Florida, Maryland and other states.Aside from civil liberties concerns, however, a For example, when researchers in the US examined how PredPol predicts crime, they found something disturbing. The software apparently sets off a feedback loop that leads to law enforcement being dispatched repeatedly to certain neighborhoods--regardless of the actual crime rates in those neighborhoods.According to "The problem stems from the logic that PredPol uses to decide where officers should be sent. If an officer is sent to a neighbourhood and then makes an arrest, the software takes this as indicating a good chance of more crimes in that area in future.What this means, says Matt Kusner at the Alan Turing Institute in London, is that the PredPol system seems to be learning from reports recorded by the police  which may be higher in areas where there are more police  rather than from underlying crime rates."Thats how dangerous feedback loops are, Joshua Loftus, Assistant Professor of Information, Operations and Management Sciences, said. These loops are only part of how PredPol makes its predictions, he said, but they may explain why predictive policing algorithms have sometimes appeared to recreate exactly the type of biases the software developers say they overcome.So, the crime rate in one neighborhood is overestimated, without taking into account the It may be possible to terminate the feedback loop, though. Loftus also indicated there are several other issues that need to be resolved before policing algorithms can truly be considered unbiased. Human decisions affect every aspect of the design of the system, he cautioned.